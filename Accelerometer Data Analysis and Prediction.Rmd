---
title: "Accelerometer Data Analysis and Prediction"
output: html_document
date: "2024-01-17"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Overview
In this project, we will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to predict the manner in which they did the exercise. This is the “classe” variable in the training set. We train 3 models: Decision Tree, Random Forest, Support Vector Machine using k-folds cross validation on the training set. We then predict using a validation set randomly selected from the training csv data to obtain the accuracy and out of sample error rate. Based on those numbers, we decide on the best model, and use it to predict 20 cases using the test csv set.

#Background
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, the goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.

The training data for this project are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

#Data Processing
##Loading Necessary Libraries
```{r}
# Load Libraries
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(corrplot)
library(ggplot2)

```

##Loading Data
```{r}
# read data from csv files
train <- read.csv("pml-training.csv", na.strings=c("NA","#DIV/0!",""))
test <- read.csv("pml-testing.csv", na.strings=c("NA","#DIV/0!",""))

head(train,n=5)
head(test,n=5)

names(train)
dim(train)
dim(test)
```
The dataset contains a lot of NA and missing values , therefore it is necessary to perform some data cleaning before proceeding with the training of models.

##Data Cleaning
```{r}
# Clean dataset by removing rows and columns with missing values
trainClean <- train[, colSums(is.na(train)) == 0] 
testClean <- test[, colSums(is.na(test)) == 0] 

trainClean <- trainClean[,-c(1:7)] #removing metadata which is irrelevant to the outcome
nearZeroVar(trainClean) # no variable with near zero variance
```

Now that the data is clean and has all the unwanted variables removed, we can split the data for training the model.

```{r}
# split data into train and test from the training set
set.seed(42)
inTrain <- createDataPartition(y=trainClean$classe, p=0.6, list=FALSE)
myTrain <- trainClean[inTrain, ]
myTest <- trainClean[-inTrain, ]
dim(myTrain)
dim(myTest)
```

##Create Regression Models
We will create 3 Regression models and perform a comparative analysis for all three of them:
```{r}
# setting control variable for 4 fold cross validation
control <- trainControl(method="cv", number=4, verboseIter=F)
```

###1.Decision Trees
```{r}
# Fitting Decision Tree Model
mod1 <- train(classe~., data=myTrain, method="rpart", trControl = control, tuneLength = 5)
rpart.plot(mod1$finalModel)
pred1 <- predict(mod1, myTest)
cm1 <- confusionMatrix(pred1, factor(myTest$classe))
cm1
plot(mod1)
```

###2.Random Forest
```{r}
# Fitting Random Forest Model
mod2 <- train(classe~., data=myTrain, method="rf", trControl = control, tuneLength = 5)
pred2 <- predict(mod2, myTest)
cm2 <- confusionMatrix(pred2, factor(myTest$classe))
cm2
plot(mod2)
```

###2.Support Vector Machine
```{r}
# Fitting SVM Model
mod3 <- train(classe~., data=myTrain, method="svmLinear", trControl = control, tuneLength = 5, verbose = F)
pred3 <- predict(mod3, myTest)
cm3 <- confusionMatrix(pred3, factor(myTest$classe))
cm3
```

##Comparative Analysis
Now, we will compare the accuracy and Out of Sample Error for all the three models
```{r}
# Compile Model Results
ModelName <- c("Decision Tree","Random Forest","SVM")
OOSE <- c(1 - as.numeric(cm1$overall[1]),1 - as.numeric(cm2$overall[1]),1 - as.numeric(cm3$overall[1]))
Accuracy <- c(0.5442,0.9944,0.7822)
Model_Summary <- data.frame(ModelName,Accuracy,OOSE)
Model_Summary
```

It can be observed that Random Forest model performs the best with the highest accuracy of 99.44%. 
Therefore, it is chosen as the best model for this dataset and the prediction on testing dataset will be performed using the Random Forest Model

```{r}
# Prediction on Tests set
pred <- predict(mod2, testClean)
print(pred)
```

